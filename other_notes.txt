Readme to help me understand stuff on this machine.
-Daniel Seita

This computer is for capturing images of the da vinci robot's camera. We can do
a lot of stuff with that.

The "endoscope" code seems to be in:

/home/davinci2/gscam/catkin_ws/src/gscam/examples/

The 'ros' startup script looks to be in /.ros and we can run that? That's where
the frames were saved by default. When Sanjay started it, he was in the
`~/.ros/log` directory and ran the following command:

roslaunch gscam endoscope.launch

Simply do CTRL-C to exit the program.

Here's the workflow for me:

(0) Actually, before we do anything I think the other computer (the "controls")
has to be started to get the robot working at all. That's the other computer but
fortunately it'll probably be on for all of my work time. I'd rather not get
involved in that.

(1) Get the device with the clips (the Stewart Platform?) set up at the center
of the robot's camera (the endoscope).

(2) Now pick a gauze and clip it inside. The gauze may or may not have different
deformations in it, which I'll do manually. However, I'll want to start with a
clean Gauze and maybe if the line to be cut is not rotationally symmetric, I'll
rotate around to get more data. But regardless, have some gauze, deformed or
not, inside there.

(3) Launch the script `roslaunch gscam endoscope.launch`. I'll see images. RIGHT
CLICK to save. I should probably do this for ONE of the left or right hand
sides. The problem is that the left/right will override each other, each time I
click on the left camera, it'll save starting from 0, but so will the right ...
seems really bad design but maybe the images are the same. UPDATE: no they're
not exactly the same, also we need both in order to measure depth correctly.

(4) Change the gauze somehow (either deform it or start with a new one) and then
take more pictures. Ways to change: 

- translate the gauze to get lighting changes
- push the gauze up/down so we see slight hills and valleys
- change the stretching, stretch different parts
- add more deforming (second-to-last resort)
- change gauze (last resort)

Repeat this until I'm satisfied.

By the way, we're only doing this with circle gauzes. It takes a lot of
man-hours to manually generate datasets like these.

(5) Now copy those over to my directory and then I can work with these. I'll
have to ssh probably, and DON'T INSTALL ANYTHING NEW WITHOUT USING VIRTUALENV.
You know, since I won't be able to do a lot manually, it's probably better if I
copy and paste it locally or to my GPU work station.

By "work with these" I mean I'm taking these JPG images and applying various
numpy array operations to deform them. Look at this online, there's tons of
stuff. I'll also be using these on *patches* not necessarily the real images. Or
think of bounding boxes? Lots of work has been done on that. Time to brainstorm
this weekend. UPDATE: no need for me to do data augmentation! There's plenty of
images as it is and performance is great.



Wednesday, January 25, 2017

NEXT STEP: Try to translate between robot frame and centroids. To be specific,
the robot must look at an image in real time and extract various patches. Then
we need to somehow obtain the center of those frames, for the robot to aim at in
**its own coordinates**. Hmmm ... maybe its better to try and get this done
independent of the robot? Never mind let's use the robot while we have it. So
goal: let's `subscribe` to an image, so write a script that when we run once
will do this process.

For real time, just call this multiple times every x seconds?

OK, I see an image_saver.py script that will save **very often** by default,
every `tick`. However, I can slow this down with `rospy.sleep(...)` which sleeps
for some number of seconds, and exit with `rospy.signal_shutdown()` once some
condition is met. The `rospy.spin()` command continually "runs" the subscribers.

Regarding patches, I can easily get patches ... and their centers (just add x,y
by dx/2 and dy/2) and those themselves will be easy to input into the trained
network. But still major questions:

1. Given pixel location out of (1080,1920)-dim RGB image, how do I get the
robot's frame coordinates? I assume once I have that, we can order the robot to
move a gripper there. My *guess* for this one is to use the camera calibration
by making a new subscriber. Fortunately, this only saves once. Then there's a
`CameraInfo` page on the Wiki which explains the coordinate system from the
camera.

2. How do I utilize the subscribers? Idea: subscribers will continually extract
numpy arrays so maybe repeatedly call the network **within** the subscriber's
method? I have to be careful that this image conversion will create consistent
input to the network as the training data. But I think the default rgb8 should
do a good job?

3. How do I actually put all this together into a policy? (Later)

Sanjay plans to get the code for turning camera point into robot frame by the
end of this week. In the meantime, I'm supposed to be doing a lot of other
stuff. I'll put the virtual environment into the `dan_seita` directory for now.
Yeah, sorry that's lame but it'll do for a project directory.



Monday, January 30, 2017 (better pick up the pace!!)

Sanjay was busy last weekend.

I think we can follow this design:

1. davinci_master.py is the code with the DavinciMaster class which we create to
run things.  It has subscribers which will repeatedly query images and then run
the network code on them.

2. utilities.py will be a script with utilities we can import for handling logic
on images, plotting, etc.  That's probably all we need.

So what I need to do before the lab meeting is to figure out more about the
"other stuff I have to do".



Thursday, February 2, 2017 (PICK UP THE PACE)

I managed to get a third gauze to create the entirely held-out test set (this is
the correct way to do data augmentation). I am getting >99% accuracy. =)
Actually, I get 123/124 using Tensorflow and 122/124 using Theano. The
predictions are almost always correct with 0.99 or higher for one class and 0.01
or less for the other class, but there are two exceptions. With Tensorflow, the
erroneous one gets:

[ 0.9967379   0.00326213]

i.e. the *reverse*. And there is another ambiguous case which is close, though
Tensorflow still gets it correct:

[ 0.34936979  0.65063024]

Now let's test davinci_master.py and the corresponding utilities.py helper file.

PS: 0 (the first index) means normal, 1 (the second index) means deformed.

OK!!! I got da vinci to work with the CNN. That is, if I run

    roslaunch gscam endoscope.launch

and then run

    python scripts/davinci_master.py

I can load in the CNN which I trained earlier, create patches based on
size and stride parameters, and then predict them. Nice! However, an
uncomfortable majority of predictions put lots of weight to the deformed case.
This might be an issue with certain patches not being representative of the
data, i.e. patches that have the gripper or those which are not entirely
contained inside the gauze.

WAIT ... I forgot to zero-center the images. That might do it!! Also it looks
like the camera image doesn't capture the full gauze ... that's interesting (and
better for our purposes since we don't have to crop ourselves).

UPDATE: yes, it was the lack of zero-centering that produced the nonsensical
results earlier. Now I run it and the results make a lot of sense. =)



Friday, February 3, 2017

Now what should I be doing? The ultimate goal ASAP is to be able to convert from
a camera frame coordinate into an actual robot movement. For the actual robot
movement, I think that needs the `robot` library but it doesn't seem to be
installed on the computer I'm using and when I move the file there (according to
GitHub tutorials) I'm getting `PyKDL not found` errors. It looks like the
*other* computer uses that library ... I'm confused again, what's the role of
these two computers?

OK maybe it's best to just figure out the correct (x,y,z) location of a patch
centroid, in terms of the robot frame. I think that's the critical part (then
once we know the frame, just use `psm1.move_cartesian_frame(...)`).

How to register these images with the robot frame? What's the difference between
this task and what Sanjay did with the primitives he defined? I'm still

I'm really confused. Let me just figure out a way to measure "disparity" among
the left/right camera images on the chessboard.



Monday, February 6, 2017

Again, what should I be doing? I'm still confused and I think it would be a lot
faster if someone else on the team did this since they'd be more familiar with
the robot. Brijen sent me an email about a camera calibration tutorial, but the
cameras are already calibrated, right? I don't think I need to read this but
maybe it will give more intuition.

OK, after settling down a bit I read Sanjay's test script and it's actually
based on Brijen's code so no need to worry, I'll read Sanjay's code. It's in
the `dvrk_calibration` directory so don't forget! It collects data points using
the usual rospy subscribers with `StereoChessboardCollect`. Then it will save
points (25 of them, apparently) into `endoscope_pts.p`. These are "camera
points" in the camera calibration.

Then we run the **robot** calibration, which saves 25 points again into
`PSM1.p`. This and the other one are 3-D points.

I think if I need any more camera and/or robot points, I should re-run this
code Sanjay has. However, it uses a specialized getCheckerBoardCorners() which
... seems too specific to the method? How do we get camera points anyway? Are
those simple pixels in which case it's not bad? Or do we collect from the
robot's arm?

So how about this, when dvrk is running and ready, in the rospy code:

- Load in camera_matrix.p and robot_matrix.p. These should be pre-computed,
  right? (And I should have some method which copies Sanjay's primitive.)

- Then have a subscriber read in left/right images (of the full camera's view),
  this is what I had before.

- Then have those two images split into patches and centroids, and extract the
  2-D **camera** point of each centroid. Is this as simple as taking the numpy
  array of pixels and finding the row and column index? I will have to ask
  someone.

- Then use the _get_points_3d(left,right) where left=row index, right=column
  index to find the 3-D point, which we then input to camera2robot to find the
  corresponding robot frame.

- Then (somewhere else, on other computer?) use the real robot code to command
  it to move to those robot points. (Where does the TRPO algorithm go?)

I hope this is right ...

EDIT: Argh, we really have to be consistent in how high we're setting the gauze.
I think it's there on the Stewart platform. So maybe be consistent: put the
gauze and its clippers and plastic circle and put it on top of a Stewart
platform which is pushed down completely with three nails holding it in place. I
hope this won't adversely affect the network's performance. By the way, for
lighting, I've always had it at 40%.

For height, I'm using the smallest chessboard piece and putting it on top of
some loose substance to get it equal to the clipper+plastic supports. Whew.
Let's try that (but don't overwrite the current stuff).

OH ok I can get findChessboardCorners(...) to work on the larger chessboard,
**assuming** that it's not too high up. I had to discard the Stewart platform.
Now I'm getting stuff that indeed looks like pixel values. With a (6,6) chess
board, you have to provide a (5,5) pairing which subtracts one, since we're only
interested in finding the corners.

Yes, I get pixels there ... but they are not aligned with the actual locations?
For both the left/right, the actual coordinates I'm getting from
`findChessboardCorners` do not align with the actual corners? It looks like I
could get them to do so by translating them somehow ... anyway I will have to
figure this out later.
