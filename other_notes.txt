Readme to help me understand stuff on this machine.
-Daniel Seita

This computer is for capturing images of the da vinci robot's camera. We can do
a lot of stuff with that.

The "endoscope" code seems to be in:

/home/davinci2/gscam/catkin_ws/src/gscam/examples/

The 'ros' startup script looks to be in /.ros and we can run that? That's where
the frames were saved by default. When Sanjay started it, he was in the
`~/.ros/log` dirctory and ran the following command:

roslaunch gscam endoscope.launch

Simply do CTRL-C to exit the program.

Here's the workflow for me:

(0) Actually, before we do anything I think the other computer (the "controls")
has to be started to get the robot working at all. That's the other computer but
fortunately it'll probably be on for all of my work time. I'd rather not get
involved in that.

(1) Get the device with the clips (the Stewart Platform?) set up at the center
of the robot's camera (the endoscope).

(2) Now pick a gauze and clip it inside. The gauze may or may not have different
deformations in it, which I'll do manually. However, I'll want to start with a
clean Gauze and maybe if the line to be cut is not rotationally symmetric, I'll
rotate around to get more data. But regardless, have some gauze, deformed or
not, inside there.

(3) Launch the script `roslaunch gscam endoscope.launch`. I'll see images. RIGHT
CLICK to save. I should probably do this for ONE of the left or right hand
sides. The problem is that the left/right will override each other, each time I
click on the left camera, it'll save starting from 0, but so will the right ...
seems really bad design but maybe the images are the same.

(4) Change the gauze somehow (either deform it or start with a new one) and then
take more pictures. Ways to change: 

- translate the gauze to get lighting changes
- push the gauze up/down so we see slight hills and valleys
- change the stretching, stretch different parts
- add more deforming (second-to-last resort)
- change gauze (last resort)

Repeat this until I'm satisfied.

By the way, we're only doing this with circle gauzes. It takes a lot of
man-hours to manually generate datsets like these.

(5) Now copy those over to my directory and then I can work with these. I'll
have to ssh probably, and DON'T INSTALL ANYTHING NEW WITHOUT USING VIRTUALENV.
You know, since I won't be able to do a lot manually, it's probably better if I
copy and paste it locally or to my GPU work station.

By "work with these" I mean I'm taking these JPG images and applying various
numpy array operations to deform them. Look at this online, there's tons of
stuff. I'll also be using these on *patches* not necessarily the real images. Or
think of bounding boxes? Lots of work has been done on that. Time to brainstorm
this weekend.



Wednesday, January 25, 2017

NEXT STEP: Try to translate between robot frame and centroids. To be specific,
the robot must look at an image in real time and extract various patches. Then
we need to somehow obtain the center of those frames, for the robot to aim at in
**its own coordinates**. Hmmm ... maybe its better to try and get this done
independent of the robot? Never mind let's use the robot while we have it. So
goal: let's `subscribe` to an image, so write a script that when we run once
will do this process.

For real time, just call this multiple times every x seconds?

OK, I see an image_saver.py script that will save **very often** by default,
every `tick`. However, I can slow this down with `rospy.sleep(...)` which sleeps
for some number of seconds, and exit with `rospy.signal_shutdown()` once some
condition is met. The `rospy.spin()` command continually "runs" the subscribers.

Regarding patches, I can easily get patches ... and their centers (just add x,y
by dx/2 and dy/2) and those themselves will be easy to input into the trained
network. But still major questions:

1. Given pixel location out of (1080,1920)-dim RGB image, how do I get the
robot's frame coordinates? I assume once I have that, we can order the robot to
move a gripper there. My *guess* for this one is to use the camera calibration
by making a new subscriber. Fortunately, this only saves once. Then there's a
`CameraInfo` page on the Wiki which explains the coordinate system from the
camera.

2. How do I utilize the subscribers? Idea: subscribers will continually extract
numpy arrays so maybe repeatedly call the network **within** the subscriber's
method? I have to be careful that this image conversion will create consistent
input to the network as the training data. But I think the default rgb8 should
do a good job?

3. How do I actually put all this together into a policy? (Later)

Sanjay plans to get the code for turning camera point into robot frame by the
end of this week. In the meantime, I'm supposed to be doing a lot of other
stuff. I'll put the virtual environment into the `dan_seita` directory for now.
Yeah, sorry that's lame but it'll do for a project directory.



Monday, January 30, 2017 (better pick up the pace!!)

Sanjay was busy last weekend.

I think we can follow this design:

1. davinci_master.py is the code with the DavinciMaster class which we create to
run things.  It has subscribers which will repeatedly query images and then run
the network code on them.

2. utilities.py will be a script with utilities we can import for handling logic
on images, plotting, etc.  That's probably all we need.

So what I need to do before the lab meeting is to figure out more about the
"other stuff I have to do".



Thursday, February 2, 2017 (PICK UP THE PACE)

I managed to get a third gauze to create the entirely held-out test set (this is
the correct way to do data augmentation). I am getting >99% accuracy. =)
Actually, I get 123/124 using Tensorflow and 122/124 using Theano. The
predictions are almost always correct with 0.99 or higher for one class and 0.01
or less for the other class, but there are two exceptions. With Tensorflow, the
erroneous one gets:

[ 0.9967379   0.00326213]

i.e. the *reverse*. And there is another ambiguous case which is close, though
Tensorflow still gets it correct:

[ 0.34936979  0.65063024]

Now let's test davinci_master.py and the corresponding utilities.py helper file.

PS: 0 (the first index) means normal, 1 (the second index) means deformed.

OK!!! I got da vinci to work with the CNN. That is, if I run

    roslaunch gscam endoscope.launch

and then run

    python scripts/davinci_master.py

I can load in the CNN which I trained earlier, create patches based on
size and stride parameters, and then predict them. Nice! However, an
uncomfortable majority of predictions put lots of weight to the deformed case.
This might be an issue with certain patches not being representative of the
data, i.e. patches that have the gripper or those which are not entirely
contained inside the gauze.

WAIT ... I forgot to zero-center the images. That might do it!! Also it looks
like the camera image doesn't capture the full gauze ... that's interesting (and
better for our purposes since we don't have to crop ourselves).

UPDATE: yes, it was the lack of zero-centering that produced the nonsensical
results earlier. Now I run it and the results make a lot of sense.
